import numpy as np\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nimport asyncio\nfrom ...utils.logger import setup_logger\n\nlogger = setup_logger(__name__)\n\n@dataclass\nclass RLAction:\n    action_type: str  # 'hold', 'buy', 'sell'\n    intensity: float  # 0.0 to 1.0\n    confidence: float\n    expected_reward: float\n\nclass CryptoTradingEnvironment:\n    \"\"\"Advanced RL environment for crypto trading (NEW - HIGHEST IMPACT)\"\"\"\n    \n    def __init__(self, market_data, initial_balance=100000):\n        self.market_data = market_data\n        self.initial_balance = initial_balance\n        self.current_step = 0\n        self.balance = initial_balance\n        self.position = 0\n        self.trades = []\n        self.max_drawdown = 0\n        self.peak_value = initial_balance\n        \n        # Enhanced action space: [hold, buy_25%, buy_50%, buy_100%, sell_25%, sell_50%, sell_100%]\n        self.action_space = 7\n        \n        # Enhanced state space with microstructure and alternative data\n        self.state_size = 75\n        \n        # Risk management\n        self.max_position_size = 0.95  # Max 95% of capital\n        self.transaction_cost = 0.001  # 0.1% transaction cost\n        \n    def reset(self):\n        \"\"\"Reset environment to initial state\"\"\"\n        self.current_step = 60  # Start after warmup period\n        self.balance = self.initial_balance\n        self.position = 0\n        self.trades = []\n        self.max_drawdown = 0\n        self.peak_value = self.initial_balance\n        return self._get_state()\n    \n    def step(self, action):\n        \"\"\"Execute action and return next state, reward, done\"\"\"\n        # Execute action\n        reward = self._execute_action(action)\n        \n        # Move to next step\n        self.current_step += 1\n        \n        # Check if episode is done\n        current_value = self.get_portfolio_value()\n        drawdown = (self.peak_value - current_value) / self.peak_value\n        \n        done = (self.current_step >= len(self.market_data) - 1 or \n                drawdown > 0.5 or  # 50% drawdown limit\n                current_value <= self.initial_balance * 0.1)  # 90% loss limit\n        \n        next_state = self._get_state() if not done else np.zeros(self.state_size)\n        \n        return next_state, reward, done, self._get_info()\n    \n    def _get_state(self):\n        \"\"\"Get comprehensive market state with enhanced features\"\"\"\n        if self.current_step >= len(self.market_data):\n            return np.zeros(self.state_size)\n        \n        # Price features (last 20 periods)\n        price_window = self.market_data['close'][max(0, self.current_step-20):self.current_step]\n        if len(price_window) < 20:\n            price_window = np.pad(price_window, (20-len(price_window), 0), 'edge')\n        \n        price_returns = np.diff(np.log(price_window + 1e-8))\n        \n        # Technical indicators\n        current_data = self.market_data.iloc[self.current_step]\n        technical_features = [\n            current_data.get('rsi_14', 50) / 100,  # Normalized RSI\n            current_data.get('macd', 0) / 100,      # Normalized MACD\n            current_data.get('bb_position', 0.5),   # Bollinger Band position\n            current_data.get('atr', 0) / current_data.get('close', 1),  # Normalized ATR\n            current_data.get('volume_ratio', 1),    # Volume ratio\n        ]\n        \n        # Market microstructure (enhanced)\n        microstructure_features = [\n            current_data.get('order_flow_imbalance', 0),\n            current_data.get('bid_ask_spread', 0) / current_data.get('close', 1),\n            current_data.get('pressure_ratio', 1),\n            current_data.get('depth_imbalance', 0),\n            current_data.get('estimated_price_impact', 0)\n        ]\n        \n        # Alternative data features\n        alt_data_features = [\n            current_data.get('social_sentiment', 0),\n            current_data.get('fear_greed_index', 50) / 100,\n            current_data.get('whale_movements', 0),\n            current_data.get('exchange_flows', 0)\n        ]\n        \n        # Portfolio state\n        current_price = current_data.get('close', price_window[-1])\n        portfolio_value = self.get_portfolio_value()\n        position_ratio = (self.position * current_price) / portfolio_value if portfolio_value > 0 else 0\n        cash_ratio = self.balance / portfolio_value if portfolio_value > 0 else 1\n        \n        # Performance metrics\n        total_return = (portfolio_value - self.initial_balance) / self.initial_balance\n        current_drawdown = (self.peak_value - portfolio_value) / self.peak_value if self.peak_value > 0 else 0\n        \n        # Market regime features\n        volatility = np.std(price_returns[-10:]) if len(price_returns) >= 10 else 0\n        trend = (current_price - price_window[0]) / price_window[0] if price_window[0] > 0 else 0\n        momentum = np.mean(price_returns[-5:]) if len(price_returns) >= 5 else 0\n        \n        # Multi-timeframe features\n        mtf_features = [\n            current_data.get('htf_trend', False),\n            current_data.get('mtf_trend', False),\n            current_data.get('stf_trend', False),\n            current_data.get('trend_agreement', 0),\n            current_data.get('momentum_divergence', 0)\n        ]\n        \n        # Recent trading performance\n        recent_trades = self.trades[-10:] if len(self.trades) >= 10 else self.trades\n        if recent_trades:\n            avg_trade_return = np.mean([t.get('return', 0) for t in recent_trades])\n            trade_win_rate = np.mean([1 if t.get('return', 0) > 0 else 0 for t in recent_trades])\n        else:\n            avg_trade_return = 0\n            trade_win_rate = 0.5\n        \n        # Combine all features\n        state = np.concatenate([\n            price_returns[-15:],  # 15 recent returns\n            technical_features,   # 5 technical indicators\n            microstructure_features,  # 5 microstructure features\n            alt_data_features,    # 4 alternative data features\n            mtf_features,         # 5 multi-timeframe features\n            [\n                position_ratio, cash_ratio, total_return, current_drawdown,\n                volatility, trend, momentum, avg_trade_return, trade_win_rate,\n                len(self.trades) / 1000,  # Normalized trade count\n                self.current_step / len(self.market_data)  # Progress through data\n            ]  # 11 portfolio/performance features\n        ])\n        \n        # Pad or truncate to exact state size\n        if len(state) < self.state_size:\n            state = np.pad(state, (0, self.state_size - len(state)), 'constant')\n        else:\n            state = state[:self.state_size]\n        \n        return state.astype(np.float32)\n    \n    def _execute_action(self, action):\n        \"\"\"Execute trading action with enhanced logic\"\"\"\n        current_price = self.market_data['close'].iloc[self.current_step]\n        \n        # Define action mappings with risk controls\n        if action == 0:  # Hold\n            trade_size = 0\n        elif action == 1:  # Buy 25%\n            available_capital = self.balance * 0.25\n            trade_size = min(available_capital / current_price, \n                           (self.initial_balance * self.max_position_size - self.position * current_price) / current_price)\n        elif action == 2:  # Buy 50%\n            available_capital = self.balance * 0.50\n            trade_size = min(available_capital / current_price,\n                           (self.initial_balance * self.max_position_size - self.position * current_price) / current_price)\n        elif action == 3:  # Buy 100%\n            available_capital = self.balance\n            trade_size = min(available_capital / current_price,\n                           (self.initial_balance * self.max_position_size - self.position * current_price) / current_price)\n        elif action == 4:  # Sell 25%\n            trade_size = -self.position * 0.25\n        elif action == 5:  # Sell 50%\n            trade_size = -self.position * 0.50\n        else:  # Sell 100%\n            trade_size = -self.position\n        \n        # Execute trade with transaction costs\n        executed_size = 0\n        transaction_cost = 0\n        \n        if trade_size > 0:  # Buy\n            cost = trade_size * current_price\n            transaction_cost = cost * self.transaction_cost\n            total_cost = cost + transaction_cost\n            \n            if total_cost <= self.balance and trade_size > 0.001:  # Minimum trade size\n                self.balance -= total_cost\n                self.position += trade_size\n                executed_size = trade_size\n                \n                self.trades.append({\n                    'type': 'buy',\n                    'size': trade_size,\n                    'price': current_price,\n                    'cost': transaction_cost,\n                    'timestamp': self.current_step\n                })\n                \n        elif trade_size < 0:  # Sell\n            sell_amount = abs(trade_size)\n            if sell_amount <= self.position and sell_amount > 0.001:  # Minimum trade size\n                revenue = sell_amount * current_price\n                transaction_cost = revenue * self.transaction_cost\n                net_revenue = revenue - transaction_cost\n                \n                self.balance += net_revenue\n                self.position -= sell_amount\n                executed_size = trade_size\n                \n                self.trades.append({\n                    'type': 'sell',\n                    'size': sell_amount,\n                    'price': current_price,\n                    'cost': transaction_cost,\n                    'timestamp': self.current_step\n                })\n        \n        # Calculate reward\n        reward = self._calculate_sophisticated_reward(executed_size, transaction_cost)\n        \n        # Update peak value and drawdown\n        current_value = self.get_portfolio_value()\n        if current_value > self.peak_value:\n            self.peak_value = current_value\n        \n        return reward\n    \n    def _calculate_sophisticated_reward(self, executed_size, transaction_cost):\n        \"\"\"Calculate sophisticated reward function for RL training\"\"\"\n        current_value = self.get_portfolio_value()\n        \n        # Base portfolio return component\n        portfolio_return = (current_value - self.initial_balance) / self.initial_balance\n        return_component = portfolio_return * 100  # Scale up\n        \n        # Risk-adjusted return (Sharpe ratio component)\n        if len(self.trades) >= 20:\n            trade_returns = self._calculate_recent_trade_returns()\n            if len(trade_returns) > 1:\n                mean_return = np.mean(trade_returns)\n                volatility = np.std(trade_returns)\n                sharpe_component = (mean_return / (volatility + 1e-8)) * 10  # Scale up\n            else:\n                sharpe_component = 0\n        else:\n            sharpe_component = 0\n        \n        # Drawdown penalty (progressive)\n        current_drawdown = (self.peak_value - current_value) / self.peak_value if self.peak_value > 0 else 0\n        if current_drawdown > 0.05:  # 5% threshold\n            drawdown_penalty = -50 * (current_drawdown - 0.05) ** 2  # Quadratic penalty\n        else:\n            drawdown_penalty = 0\n        \n        # Transaction cost penalty\n        cost_penalty = -transaction_cost / self.initial_balance * 1000  # Scale penalty\n        \n        # Position management reward\n        position_value = self.position * self.market_data['close'].iloc[self.current_step]\n        portfolio_balance = position_value + self.balance\n        if portfolio_balance > 0:\n            position_ratio = position_value / portfolio_balance\n            # Reward balanced positions, penalize extreme allocations\n            if 0.3 <= position_ratio <= 0.8:\n                position_reward = 2\n            elif position_ratio > 0.95 or position_ratio < 0.05:\n                position_reward = -5\n            else:\n                position_reward = 0\n        else:\n            position_reward = -10\n        \n        # Trading frequency management\n        recent_trades = len([t for t in self.trades if self.current_step - t['timestamp'] <= 20])\n        if recent_trades > 10:  # More than 10 trades in 20 periods\n            frequency_penalty = -2 * (recent_trades - 10)\n        else:\n            frequency_penalty = 0\n        \n        # Momentum alignment reward\n        if len(self.trades) > 0:\n            last_trade = self.trades[-1]\n            current_price = self.market_data['close'].iloc[self.current_step]\n            \n            if last_trade['type'] == 'buy':\n                if current_price > last_trade['price']:\n                    momentum_reward = 1  # Bought before price increase\n                else:\n                    momentum_reward = -0.5\n            elif last_trade['type'] == 'sell':\n                if current_price < last_trade['price']:\n                    momentum_reward = 1  # Sold before price decrease\n                else:\n                    momentum_reward = -0.5\n            else:\n                momentum_reward = 0\n        else:\n            momentum_reward = 0\n        \n        # Combine all reward components\n        total_reward = (return_component + \n                       sharpe_component + \n                       drawdown_penalty + \n                       cost_penalty + \n                       position_reward + \n                       frequency_penalty + \n                       momentum_reward)\n        \n        return total_reward\n    \n    def _calculate_recent_trade_returns(self, window=20):\n        \"\"\"Calculate returns for recent trades\"\"\"\n        if len(self.trades) < 2:\n            return []\n        \n        returns = []\n        recent_trades = self.trades[-window:]\n        \n        # Simple return calculation based on price changes\n        for i in range(1, len(recent_trades)):\n            prev_trade = recent_trades[i-1]\n            curr_trade = recent_trades[i]\n            \n            if prev_trade['type'] == 'buy' and curr_trade['type'] == 'sell':\n                trade_return = (curr_trade['price'] - prev_trade['price']) / prev_trade['price']\n                returns.append(trade_return)\n        \n        return returns\n    \n    def get_portfolio_value(self):\n        \"\"\"Get current portfolio value\"\"\"\n        if self.current_step < len(self.market_data):\n            current_price = self.market_data['close'].iloc[self.current_step]\n        else:\n            current_price = self.market_data['close'].iloc[-1]\n        \n        return self.balance + self.position * current_price\n    \n    def _get_info(self):\n        \"\"\"Get additional information for logging/analysis\"\"\"\n        return {\n            'portfolio_value': self.get_portfolio_value(),\n            'position': self.position,\n            'balance': self.balance,\n            'num_trades': len(self.trades),\n            'max_drawdown': self.max_drawdown\n        }\n\nclass MultiAgentTradingSystem:\n    \"\"\"Multi-agent system with specialized agents (NEW - HIGHEST IMPACT)\"\"\"\n    \n    def __init__(self):\n        # Specialized agents for different strategies\n        self.agents = {\n            'trend_follower': None,\n            'mean_reverter': None,\n            'volatility_trader': None,\n            'momentum_trader': None\n        }\n        \n        # Meta-agent for allocation decisions\n        self.meta_agent = None\n        self.agent_performance = {name: [] for name in self.agents.keys()}\n        \n    def train_agents(self, market_data, training_episodes=1000):\n        \"\"\"Train specialized agents using different reward functions\"\"\"\n        logger.info(\"Training multi-agent system...\")\n        \n        for agent_name in self.agents.keys():\n            logger.info(f\"Training {agent_name}...\")\n            \n            # Create environment with agent-specific modifications\n            env = CryptoTradingEnvironment(market_data)\n            \n            # Customize reward function for each agent type\n            if agent_name == 'trend_follower':\n                env._calculate_sophisticated_reward = self._trend_following_reward\n            elif agent_name == 'mean_reverter':\n                env._calculate_sophisticated_reward = self._mean_reversion_reward\n            elif agent_name == 'volatility_trader':\n                env._calculate_sophisticated_reward = self._volatility_trading_reward\n            elif agent_name == 'momentum_trader':\n                env._calculate_sophisticated_reward = self._momentum_trading_reward\n            \n            # Simple neural network agent (placeholder for actual RL implementation)\n            agent = SimpleNeuralAgent(env.state_size, env.action_space)\n            \n            # Train agent\n            for episode in range(training_episodes):\n                state = env.reset()\n                done = False\n                episode_reward = 0\n                \n                while not done:\n                    action = agent.act(state)\n                    next_state, reward, done, info = env.step(action)\n                    agent.remember(state, action, reward, next_state, done)\n                    \n                    state = next_state\n                    episode_reward += reward\n                \n                self.agent_performance[agent_name].append(episode_reward)\n                \n                # Train agent periodically\n                if episode % 50 == 0:\n                    agent.train()\n            \n            self.agents[agent_name] = agent\n            logger.info(f\"Completed training {agent_name}\")\n    \n    def get_ensemble_action(self, state, market_conditions):\n        \"\"\"Get weighted ensemble action from all agents\"\"\"\n        if not all(self.agents.values()):\n            return 0  # Default to hold if agents not trained\n        \n        agent_actions = {}\n        agent_confidences = {}\n        \n        for agent_name, agent in self.agents.items():\n            if agent is not None:\n                action = agent.predict(state)\n                confidence = agent.get_confidence(state)\n                agent_actions[agent_name] = action\n                agent_confidences[agent_name] = confidence\n        \n        # Calculate dynamic weights based on recent performance and market conditions\n        weights = self._calculate_agent_weights(market_conditions)\n        \n        # Weighted voting with confidence adjustment\n        action_scores = {}\n        for agent_name, action in agent_actions.items():\n            weight = weights.get(agent_name, 0)\n            confidence = agent_confidences.get(agent_name, 0.5)\n            \n            adjusted_weight = weight * confidence\n            \n            if action not in action_scores:\n                action_scores[action] = 0\n            action_scores[action] += adjusted_weight\n        \n        # Return action with highest weighted score\n        if action_scores:\n            best_action = max(action_scores.items(), key=lambda x: x[1])[0]\n            return best_action\n        \n        return 0  # Default to hold\n    \n    def _calculate_agent_weights(self, market_conditions):\n        \"\"\"Calculate dynamic weights for agents based on market conditions\"\"\"\n        # Get recent performance\n        recent_window = 50\n        performance_scores = {}\n        \n        for agent_name, performance in self.agent_performance.items():\n            if len(performance) >= recent_window:\n                recent_perf = performance[-recent_window:]\n                avg_performance = np.mean(recent_perf)\n                performance_scores[agent_name] = max(0, avg_performance)  # Only positive performance\n            else:\n                performance_scores[agent_name] = 0\n        \n        # Market condition adjustments\n        volatility = market_conditions.get('volatility', 0.02)\n        trend_strength = market_conditions.get('trend_strength', 0)\n        \n        # Base weights\n        weights = {\n            'trend_follower': 0.25,\n            'mean_reverter': 0.25,\n            'volatility_trader': 0.25,\n            'momentum_trader': 0.25\n        }\n        \n        # Adjust based on market conditions\n        if volatility > 0.04:  # High volatility\n            weights['volatility_trader'] *= 1.5\n            weights['mean_reverter'] *= 0.7\n        elif volatility < 0.015:  # Low volatility\n            weights['mean_reverter'] *= 1.4\n            weights['volatility_trader'] *= 0.6\n        \n        if abs(trend_strength) > 0.02:  # Strong trend\n            weights['trend_follower'] *= 1.3\n            weights['momentum_trader'] *= 1.2\n            weights['mean_reverter'] *= 0.6\n        \n        # Adjust based on recent performance\n        total_performance = sum(performance_scores.values())\n        if total_performance > 0:\n            for agent_name in weights.keys():\n                performance_weight = performance_scores[agent_name] / total_performance\n                weights[agent_name] = (weights[agent_name] * 0.7 + performance_weight * 0.3)\n        \n        # Normalize weights\n        total_weight = sum(weights.values())\n        if total_weight > 0:\n            weights = {k: v / total_weight for k, v in weights.items()}\n        \n        return weights\n    \n    def _trend_following_reward(self, executed_size, transaction_cost):\n        \"\"\"Reward function optimized for trend following\"\"\"\n        # Simplified trend-following reward\n        return 0  # Placeholder\n    \n    def _mean_reversion_reward(self, executed_size, transaction_cost):\n        \"\"\"Reward function optimized for mean reversion\"\"\"\n        # Simplified mean reversion reward\n        return 0  # Placeholder\n    \n    def _volatility_trading_reward(self, executed_size, transaction_cost):\n        \"\"\"Reward function optimized for volatility trading\"\"\"\n        # Simplified volatility trading reward\n        return 0  # Placeholder\n    \n    def _momentum_trading_reward(self, executed_size, transaction_cost):\n        \"\"\"Reward function optimized for momentum trading\"\"\"\n        # Simplified momentum trading reward\n        return 0  # Placeholder\n\nclass SimpleNeuralAgent:\n    \"\"\"Simple neural network agent for RL (placeholder implementation)\"\"\"\n    \n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = []\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n        \n        # Simple neural network\n        self.model = nn.Sequential(\n            nn.Linear(state_size, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_size)\n        )\n        \n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n        self.criterion = nn.MSELoss()\n    \n    def remember(self, state, action, reward, next_state, done):\n        \"\"\"Store experience in memory\"\"\"\n        self.memory.append((state, action, reward, next_state, done))\n        if len(self.memory) > 2000:\n            self.memory.pop(0)\n    \n    def act(self, state):\n        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n        if np.random.random() <= self.epsilon:\n            return np.random.choice(self.action_size)\n        \n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.model(state_tensor)\n        return q_values.argmax().item()\n    \n    def predict(self, state):\n        \"\"\"Predict action without exploration\"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        with torch.no_grad():\n            q_values = self.model(state_tensor)\n        return q_values.argmax().item()\n    \n    def get_confidence(self, state):\n        \"\"\"Get confidence in prediction\"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        with torch.no_grad():\n            q_values = self.model(state_tensor)\n            confidence = torch.softmax(q_values, dim=1).max().item()\n        return confidence\n    \n    def train(self, batch_size=32):\n        \"\"\"Train the neural network\"\"\"\n        if len(self.memory) < batch_size:\n            return\n        \n        batch = np.random.choice(len(self.memory), batch_size, replace=False)\n        states = torch.FloatTensor([self.memory[i][0] for i in batch])\n        actions = torch.LongTensor([self.memory[i][1] for i in batch])\n        rewards = torch.FloatTensor([self.memory[i][2] for i in batch])\n        next_states = torch.FloatTensor([self.memory[i][3] for i in batch])\n        dones = torch.BoolTensor([self.memory[i][4] for i in batch])\n        \n        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.model(next_states).max(1)[0].detach()\n        target_q_values = rewards + (0.99 * next_q_values * ~dones)\n        \n        loss = self.criterion(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n