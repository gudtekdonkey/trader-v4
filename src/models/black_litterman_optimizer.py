import numpy as np\nimport pandas as pd\nfrom scipy import linalg\nfrom scipy.optimize import minimize\nfrom typing import Dict, List, Optional, Tuple\nfrom ...utils.logger import setup_logger\n\nlogger = setup_logger(__name__)\n\nclass BlackLittermanOptimizer:\n    \"\"\"Advanced Black-Litterman portfolio optimization for crypto assets (NEW - HIGH IMPACT)\"\"\"\n    \n    def __init__(self, risk_aversion=3.0, tau=0.025):\n        self.risk_aversion = risk_aversion\n        self.tau = tau  # Uncertainty in prior estimate\n        \n    def optimize_portfolio(self, returns_data: pd.DataFrame, \n                         market_caps: pd.Series, \n                         views: Optional[Dict] = None, \n                         view_confidences: Optional[Dict] = None,\n                         constraints: Optional[Dict] = None) -> Dict:\n        \"\"\"\n        Optimize portfolio using Black-Litterman model\n        \n        Args:\n            returns_data: DataFrame of asset returns\n            market_caps: Series of market capitalizations\n            views: Dict of investor views {asset: expected_return}\n            view_confidences: Dict of confidence levels {asset: confidence}\n            constraints: Additional portfolio constraints\n        \"\"\"\n        \n        try:\n            # Step 1: Calculate market portfolio weights (equilibrium)\n            market_weights = self._calculate_market_weights(market_caps)\n            \n            # Step 2: Calculate covariance matrix with shrinkage\n            cov_matrix = self._calculate_covariance_matrix(returns_data, method='shrinkage')\n            \n            # Step 3: Calculate implied equilibrium returns\n            equilibrium_returns = self._calculate_equilibrium_returns(market_weights, cov_matrix)\n            \n            # Step 4: Incorporate investor views\n            if views is not None and view_confidences is not None:\n                bl_returns, bl_cov = self._incorporate_views(\n                    equilibrium_returns, cov_matrix, views, view_confidences\n                )\n            else:\n                bl_returns = equilibrium_returns\n                bl_cov = cov_matrix\n            \n            # Step 5: Optimize portfolio with constraints\n            optimal_weights = self._optimize_weights(bl_returns, bl_cov, constraints)\n            \n            # Calculate portfolio metrics\n            expected_return = np.dot(optimal_weights, bl_returns)\n            portfolio_volatility = np.sqrt(np.dot(optimal_weights, np.dot(bl_cov, optimal_weights)))\n            sharpe_ratio = expected_return / portfolio_volatility if portfolio_volatility > 0 else 0\n            \n            return {\n                'weights': optimal_weights,\n                'expected_returns': bl_returns,\n                'covariance': bl_cov,\n                'expected_portfolio_return': expected_return,\n                'portfolio_volatility': portfolio_volatility,\n                'sharpe_ratio': sharpe_ratio,\n                'market_weights': market_weights,\n                'equilibrium_returns': equilibrium_returns\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error in Black-Litterman optimization: {e}\")\n            # Fallback to equal weights\n            n_assets = len(returns_data.columns)\n            fallback_weights = pd.Series(np.ones(n_assets) / n_assets, index=returns_data.columns)\n            return {\n                'weights': fallback_weights,\n                'expected_returns': returns_data.mean(),\n                'covariance': returns_data.cov(),\n                'expected_portfolio_return': returns_data.mean().mean(),\n                'portfolio_volatility': 0.02,\n                'sharpe_ratio': 0,\n                'status': 'fallback'\n            }\n    \n    def _calculate_market_weights(self, market_caps: pd.Series) -> pd.Series:\n        \"\"\"Calculate market capitalization weights\"\"\"\n        total_market_cap = market_caps.sum()\n        if total_market_cap > 0:\n            return market_caps / total_market_cap\n        else:\n            # Equal weights fallback\n            return pd.Series(np.ones(len(market_caps)) / len(market_caps), index=market_caps.index)\n    \n    def _calculate_covariance_matrix(self, returns_data: pd.DataFrame, method='shrinkage') -> pd.DataFrame:\n        \"\"\"Calculate covariance matrix with various methods\"\"\"\n        if method == 'exponential':\n            # Exponentially weighted covariance\n            return returns_data.ewm(span=60).cov().iloc[-len(returns_data.columns):]\n        elif method == 'shrinkage':\n            # Ledoit-Wolf shrinkage\n            return self._ledoit_wolf_shrinkage(returns_data)\n        else:\n            # Simple sample covariance\n            return returns_data.cov()\n    \n    def _ledoit_wolf_shrinkage(self, returns_data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Ledoit-Wolf shrinkage estimator for covariance matrix\"\"\"\n        T, N = returns_data.shape\n        \n        if T < N:\n            logger.warning(\"Not enough observations for reliable covariance estimation\")\n            return returns_data.cov()\n        \n        # Sample covariance matrix\n        S = returns_data.cov().values\n        \n        # Shrinkage target (identity matrix scaled by average variance)\n        mu = np.trace(S) / N\n        F = mu * np.eye(N)\n        \n        # Calculate shrinkage intensity\n        X = returns_data.values\n        X_centered = X - X.mean(axis=0)\n        \n        # Shrinkage intensity calculation (simplified)\n        pi_hat = np.sum((X_centered**2).T @ (X_centered**2)) / T\n        rho_hat = np.sum(np.diag((X_centered.T @ X_centered / T - S)**2))\n        gamma_hat = np.linalg.norm(S - F, 'fro')**2\n        \n        kappa = (pi_hat - rho_hat) / gamma_hat if gamma_hat > 0 else 1\n        shrinkage = max(0, min(1, kappa / T))\n        \n        # Shrunk covariance matrix\n        shrunk_cov = shrinkage * F + (1 - shrinkage) * S\n        \n        return pd.DataFrame(shrunk_cov, \n                          index=returns_data.columns, \n                          columns=returns_data.columns)\n    \n    def _calculate_equilibrium_returns(self, market_weights: pd.Series, cov_matrix: pd.DataFrame) -> pd.Series:\n        \"\"\"Calculate implied equilibrium returns\"\"\"\n        # Reverse optimization: mu = lambda * Sigma * w\n        return self.risk_aversion * np.dot(cov_matrix, market_weights)\n    \n    def _incorporate_views(self, equilibrium_returns: pd.Series, cov_matrix: pd.DataFrame, \n                          views: Dict, view_confidences: Dict) -> Tuple[pd.Series, pd.DataFrame]:\n        \"\"\"Incorporate investor views using Bayesian updating\"\"\"\n        \n        # Construct picking matrix P and view vector Q\n        assets = equilibrium_returns.index\n        n_views = len(views)\n        P = np.zeros((n_views, len(assets)))\n        Q = np.zeros(n_views)\n        \n        valid_views = 0\n        for i, (asset, view_return) in enumerate(views.items()):\n            if asset in assets:\n                asset_idx = assets.get_loc(asset)\n                P[valid_views, asset_idx] = 1\n                Q[valid_views] = view_return\n                valid_views += 1\n        \n        if valid_views == 0:\n            logger.warning(\"No valid views found, returning equilibrium values\")\n            return equilibrium_returns, cov_matrix\n        \n        # Trim to valid views\n        P = P[:valid_views, :]\n        Q = Q[:valid_views]\n        \n        # Construct view uncertainty matrix Omega\n        Omega = np.diag([1/view_confidences.get(asset, 1.0) for asset in list(views.keys())[:valid_views]])\n        \n        # Bayesian updating\n        tau_sigma = self.tau * cov_matrix.values\n        \n        try:\n            # Calculate Black-Litterman expected returns\n            M1 = linalg.inv(tau_sigma)\n            M2 = np.dot(P.T, np.dot(linalg.inv(Omega), P))\n            M3 = np.dot(M1, equilibrium_returns.values)\n            M4 = np.dot(P.T, np.dot(linalg.inv(Omega), Q))\n            \n            mu_bl = np.dot(linalg.inv(M1 + M2), M3 + M4)\n            \n            # Calculate Black-Litterman covariance matrix\n            cov_bl = linalg.inv(M1 + M2)\n            \n            return (pd.Series(mu_bl, index=assets), \n                   pd.DataFrame(cov_bl, index=assets, columns=assets))\n            \n        except linalg.LinAlgError as e:\n            logger.warning(f\"Matrix inversion failed: {e}. Using equilibrium values.\")\n            return equilibrium_returns, cov_matrix\n    \n    def _optimize_weights(self, expected_returns: pd.Series, cov_matrix: pd.DataFrame, \n                         constraints: Optional[Dict] = None) -> pd.Series:\n        \"\"\"Optimize portfolio weights with constraints\"\"\"\n        n_assets = len(expected_returns)\n        \n        # Default constraints for crypto\n        default_constraints = {\n            'max_weight': 0.4,  # Max 40% per asset\n            'min_weight': 0.0,  # No short selling\n            'max_concentration': 0.6,  # Max 60% in top 3 assets\n            'min_diversification': 3  # Minimum 3 assets\n        }\n        \n        if constraints:\n            default_constraints.update(constraints)\n        \n        # Objective function: maximize utility = expected_return - 0.5 * risk_aversion * variance\n        def objective(weights):\n            portfolio_return = np.dot(weights, expected_returns)\n            portfolio_variance = np.dot(weights, np.dot(cov_matrix, weights))\n            return -(portfolio_return - 0.5 * self.risk_aversion * portfolio_variance)\n        \n        # Constraints\n        constraint_list = [\n            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # Weights sum to 1\n        ]\n        \n        # Concentration constraint\n        if 'max_concentration' in default_constraints:\n            max_conc = default_constraints['max_concentration']\n            def concentration_constraint(weights):\n                sorted_weights = np.sort(weights)[::-1]  # Descending order\n                top_3_weight = np.sum(sorted_weights[:3])\n                return max_conc - top_3_weight\n            \n            constraint_list.append({'type': 'ineq', 'fun': concentration_constraint})\n        \n        # Minimum diversification\n        if 'min_diversification' in default_constraints:\n            min_assets = default_constraints['min_diversification']\n            def diversification_constraint(weights):\n                active_assets = np.sum(weights > 0.01)  # Assets with >1% weight\n                return active_assets - min_assets\n            \n            constraint_list.append({'type': 'ineq', 'fun': diversification_constraint})\n        \n        # Bounds\n        min_weight = default_constraints.get('min_weight', 0.0)\n        max_weight = default_constraints.get('max_weight', 0.4)\n        bounds = [(min_weight, max_weight) for _ in range(n_assets)]\n        \n        # Initial guess (equal weights)\n        x0 = np.ones(n_assets) / n_assets\n        \n        # Optimize\n        try:\n            result = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraint_list)\n            \n            if result.success:\n                weights = result.x\n                # Ensure weights are non-negative and sum to 1\n                weights = np.maximum(weights, 0)\n                weights = weights / np.sum(weights)\n                return pd.Series(weights, index=expected_returns.index)\n            else:\n                logger.warning(f\"Optimization failed: {result.message}\")\n                return self._fallback_weights(expected_returns, default_constraints)\n                \n        except Exception as e:\n            logger.error(f\"Optimization error: {e}\")\n            return self._fallback_weights(expected_returns, default_constraints)\n    \n    def _fallback_weights(self, expected_returns: pd.Series, constraints: Dict) -> pd.Series:\n        \"\"\"Fallback weight calculation if optimization fails\"\"\"\n        n_assets = len(expected_returns)\n        max_weight = constraints.get('max_weight', 0.4)\n        \n        # Risk parity-like allocation\n        weights = np.ones(n_assets) / n_assets\n        \n        # Cap maximum weights\n        weights = np.minimum(weights, max_weight)\n        weights = weights / np.sum(weights)\n        \n        return pd.Series(weights, index=expected_returns.index)\n\nclass CryptoViewGenerator:\n    \"\"\"Generate investor views for Black-Litterman optimization (NEW)\"\"\"\n    \n    def __init__(self, ml_models=None, market_data=None):\n        self.ml_models = ml_models or {}\n        self.market_data = market_data\n        \n    def generate_ml_views(self, confidence_threshold=0.6) -> Tuple[Dict, Dict]:\n        \"\"\"Generate views based on ML predictions\"\"\"\n        views = {}\n        view_confidences = {}\n        \n        for symbol, model in self.ml_models.items():\n            try:\n                # Get ML prediction\n                if hasattr(model, 'predict') and self.market_data is not None:\n                    prediction = model.predict(self.market_data.get(symbol, pd.DataFrame()))\n                    \n                    if isinstance(prediction, dict):\n                        confidence = prediction.get('confidence', 0.5)\n                        expected_return = prediction.get('expected_return', 0)\n                        \n                        if confidence >= confidence_threshold:\n                            views[symbol] = expected_return\n                            view_confidences[symbol] = confidence\n                            \n            except Exception as e:\n                logger.warning(f\"Error generating view for {symbol}: {e}\")\n                continue\n        \n        return views, view_confidences\n    \n    def generate_technical_views(self, technical_data: pd.DataFrame) -> Tuple[Dict, Dict]:\n        \"\"\"Generate views based on technical analysis\"\"\"\n        views = {}\n        view_confidences = {}\n        \n        for symbol in technical_data.columns:\n            try:\n                # Get technical indicators\n                symbol_data = technical_data[symbol]\n                \n                # RSI-based view\n                rsi = symbol_data.get('rsi', 50)\n                if rsi < 30:  # Oversold\n                    views[symbol] = 0.05  # Expect 5% return\n                    view_confidences[symbol] = (30 - rsi) / 30  # Higher confidence for lower RSI\n                elif rsi > 70:  # Overbought\n                    views[symbol] = -0.03  # Expect -3% return\n                    view_confidences[symbol] = (rsi - 70) / 30  # Higher confidence for higher RSI\n                \n                # MACD-based view enhancement\n                macd = symbol_data.get('macd', 0)\n                if symbol in views and macd != 0:\n                    if (views[symbol] > 0 and macd > 0) or (views[symbol] < 0 and macd < 0):\n                        view_confidences[symbol] = min(0.9, view_confidences[symbol] * 1.2)\n                    \n            except Exception as e:\n                logger.warning(f\"Error generating technical view for {symbol}: {e}\")\n                continue\n        \n        return views, view_confidences\n    \n    def generate_sentiment_views(self, sentiment_data: Dict) -> Tuple[Dict, Dict]:\n        \"\"\"Generate views based on sentiment data\"\"\"\n        views = {}\n        view_confidences = {}\n        \n        for symbol, sentiment_score in sentiment_data.items():\n            try:\n                # Convert sentiment to expected return\n                # Sentiment score assumed to be between -1 and 1\n                if abs(sentiment_score) > 0.3:  # Only use strong sentiment\n                    expected_return = sentiment_score * 0.08  # Max 8% expected return\n                    confidence = abs(sentiment_score)  # Higher confidence for stronger sentiment\n                    \n                    views[symbol] = expected_return\n                    view_confidences[symbol] = confidence\n                    \n            except Exception as e:\n                logger.warning(f\"Error generating sentiment view for {symbol}: {e}\")\n                continue\n        \n        return views, view_confidences\n    \n    def combine_views(self, *view_sets) -> Tuple[Dict, Dict]:\n        \"\"\"Combine multiple sets of views\"\"\"\n        combined_views = {}\n        combined_confidences = {}\n        \n        # Collect all views for each symbol\n        symbol_views = {}\n        symbol_confidences = {}\n        \n        for views, confidences in view_sets:\n            for symbol, view in views.items():\n                if symbol not in symbol_views:\n                    symbol_views[symbol] = []\n                    symbol_confidences[symbol] = []\n                \n                symbol_views[symbol].append(view)\n                symbol_confidences[symbol].append(confidences[symbol])\n        \n        # Combine views using confidence-weighted averaging\n        for symbol in symbol_views:\n            views_list = symbol_views[symbol]\n            conf_list = symbol_confidences[symbol]\n            \n            if views_list:\n                # Weighted average of views\n                total_conf = sum(conf_list)\n                if total_conf > 0:\n                    weighted_view = sum(v * c for v, c in zip(views_list, conf_list)) / total_conf\n                    avg_confidence = total_conf / len(views_list)  # Average confidence\n                    \n                    combined_views[symbol] = weighted_view\n                    combined_confidences[symbol] = min(0.95, avg_confidence)  # Cap at 95%\n        \n        return combined_views, combined_confidences\n